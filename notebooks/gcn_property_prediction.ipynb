{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "gcn_property_prediction.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AzFXv5-BpZoe",
        "colab_type": "text"
      },
      "source": [
        "# Molecule Property Prediction with Tox21 Dataset\n",
        "\n",
        "\n",
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/deepchem/jaxchem//blob/master/notebooks/gcn_property_prediction.ipynb)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2bhhW4ekZ_ts",
        "colab_type": "text"
      },
      "source": [
        "## Install packages\n",
        "\n",
        "First, we need to install deepchem for using some useful functions about the Tox21 dataset. (Maybe it will take almost 3 minutes)\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TmtjyHIFrkKi",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 153
        },
        "outputId": "38393f55-d194-4896-d916-05f1c4cc0ae4"
      },
      "source": [
        "%tensorflow_version 1.x\n",
        "!curl -Lo deepchem_installer.py https://raw.githubusercontent.com/deepchem/deepchem/master/scripts/colab_install.py\n",
        "import deepchem_installer\n",
        "%time deepchem_installer.install(version='2.3.0')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "TensorFlow 1.x selected.\n",
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "100  3477  100  3477    0     0  17922      0 --:--:-- --:--:-- --:--:-- 17922\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "add /root/miniconda/lib/python3.6/site-packages to PYTHONPATH\n",
            "deepchem is already installed\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "CPU times: user 2.05 ms, sys: 9 Âµs, total: 2.06 ms\n",
            "Wall time: 1.69 ms\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "askrOx-kbXGk",
        "colab_type": "text"
      },
      "source": [
        "And then, we install jaxchem with some dependencies   \n",
        "**Caution** : After running the following commands, you need to restart the session. If you don't restart the session, maybe you will face an error."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Pcur1OJXIAJz",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "0cad727d-aee8-4203-f746-83c0e075aa31"
      },
      "source": [
        "!pip install -q dm-haiku==0.0.1 typing-extensions==3.7.4.2  git+https://github.com/deepchem/jaxchem"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "  Building wheel for jaxchem (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CJd7o5qNcps5",
        "colab_type": "text"
      },
      "source": [
        "## Import modules\n",
        "\n",
        "If we face the error `ImportError: cannot import name 'Literal'\n",
        "`, we should restart the session of this notebook."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2XBbmpnacoYa",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 207
        },
        "outputId": "0f6f6d65-30f9-4181-8213-cd3ec3e04403"
      },
      "source": [
        "import os\n",
        "import time\n",
        "import random\n",
        "import pickle\n",
        "import argparse\n",
        "from typing import Any, Tuple, List\n",
        "\n",
        "import jax\n",
        "import numpy as np\n",
        "import haiku as hk\n",
        "import jax.numpy as jnp\n",
        "from jax.experimental import optix\n",
        "from sklearn.metrics import roc_auc_score\n",
        "\n",
        "\n",
        "from deepchem.molnet import load_tox21\n",
        "from jaxchem.models import PadGCNPredicator as GCNPredicator\n",
        "from jaxchem.loss import binary_cross_entropy_with_logits as bce_with_logits\n",
        "from jaxchem.utils import EarlyStopping\n",
        "\n",
        "\n",
        "# type definition\n",
        "Batch = Tuple[Tuple[np.ndarray, np.ndarray], np.ndarray]\n",
        "State, OptState = Any, Any"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.\n",
            "  warnings.warn(msg, category=FutureWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:\n",
            "The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
            "For more information, please see:\n",
            "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
            "  * https://github.com/tensorflow/addons\n",
            "  * https://github.com/tensorflow/io (for I/O related ops)\n",
            "If you depend on functionality not listed there, please file an issue.\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Czw4zaO6dYWG",
        "colab_type": "text"
      },
      "source": [
        "## Download Tox21 dateset\n",
        "\n",
        "we download the Tox21 dataset which were preprocessed.  In this example, we should use the `AdjacencyConv` featurizer because `PadGCNPredicator` depends on the pad pattern GCN which use the adjacency matrix to represent node connections\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WtlinRn-f_GL",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "72648273-86ea-4562-935e-6713bbf1073b"
      },
      "source": [
        "# load tox21 dataset\n",
        "tox21_tasks, tox21_datasets, _ = load_tox21(featurizer='AdjacencyConv', reload=True)\n",
        "train_dataset, valid_dataset, test_dataset = tox21_datasets"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Loading dataset from disk.\n",
            "Loading dataset from disk.\n",
            "Loading dataset from disk.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AZWsVSABvzMF",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "1140b3a4-1059-4af6-cb79-53752906bc31"
      },
      "source": [
        "print(tox21_tasks)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['NR-AR', 'NR-AR-LBD', 'NR-AhR', 'NR-Aromatase', 'NR-ER', 'NR-ER-LBD', 'NR-PPAR-gamma', 'SR-ARE', 'SR-ATAD5', 'SR-HSE', 'SR-MMP', 'SR-p53']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VKaZhd89dEz8",
        "colab_type": "text"
      },
      "source": [
        "## Define some utilities"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "55ycWMB1dBE5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def seed_everything(seed: int = 42):\n",
        "    random.seed(seed)\n",
        "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
        "    np.random.seed(seed)\n",
        "\n",
        "\n",
        "def collate_fn(original_batch: Any) -> Batch:\n",
        "    \"\"\"Make batch data as PadGCN model inputs.\"\"\"\n",
        "    inputs, targets, _, _ = original_batch\n",
        "    node_feats = np.array([inputs[i][1] for i in range(len(inputs))])\n",
        "    adj = np.array([inputs[i][0] for i in range(len(inputs))])\n",
        "    return (node_feats, adj), np.array(targets)\n",
        "\n",
        "\n",
        "def multi_task_roc_auc_score(y_true: np.ndarray, y_score: np.ndarray) -> Tuple[float, List[float]]:\n",
        "    \"\"\"Calculate the roc_auc_score of all tasks for Tox21.\"\"\"\n",
        "    num_tasks = y_true.shape[1]\n",
        "    scores = []\n",
        "    for i in range(num_tasks):\n",
        "        scores.append(roc_auc_score(y_true[:, i], y_score[:, i]))\n",
        "    return np.mean(scores), scores"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hoqID21XceTH",
        "colab_type": "text"
      },
      "source": [
        "## Setup model and optimizer\n",
        "\n",
        "We define the forward function using `GCNPredicator` which JAXChem provides. In this case, our task is a classification, so we modify the output of  `GCNPredicator` using a sigmoid function. After defining the forward function, we create the model instance by using `haiku.transform_with_state`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GKZl6BKBdzzz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "rng_seq = hk.PRNGSequence(1234)\n",
        "\n",
        "# model params\n",
        "in_feats = train_dataset.X[0][1].shape[1]\n",
        "hidden_feats = [64, 64, 32]\n",
        "activation, batch_norm, dropout = None, None, None  # use default\n",
        "predicator_hidden_feats = 32\n",
        "pooling_method = 'mean'\n",
        "predicator_dropout = 0.2\n",
        "n_out = len(tox21_tasks)\n",
        "\n",
        "def forward(node_feats: jnp.ndarray, adj: jnp.ndarray, is_training: bool) -> jnp.ndarray:\n",
        "    \"\"\"Forward application of the GCN.\"\"\"\n",
        "    model = GCNPredicator(in_feats=in_feats, hidden_feats=hidden_feats, activation=activation,\n",
        "                          batch_norm=batch_norm, dropout=dropout, pooling_method=pooling_method,\n",
        "                          predicator_hidden_feats=predicator_hidden_feats,\n",
        "                          predicator_dropout=predicator_dropout, n_out=n_out)\n",
        "    preds = model(node_feats, adj, is_training)\n",
        "    return preds\n",
        "\n",
        "# we use haiku\n",
        "model = hk.transform_with_state(forward)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zz6mxngGaxx4",
        "colab_type": "text"
      },
      "source": [
        "And then, we also create the optimizer instance. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3QURfCI7eWD1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# optimizer params\n",
        "lr = 0.001\n",
        "optimizer = optix.adam(learning_rate=lr)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9H8SWjyfdeJX",
        "colab_type": "text"
      },
      "source": [
        "## Define loss, update and evaluate function\n",
        "\n",
        "Using the model and optimizer instance, we define the following functions. These functions are used in a training loop, so we add `@jax.jit` of the decorator to improve the performance.\n",
        "\n",
        "- The function which calculates a loss value\n",
        "- The function which updates parameters\n",
        "- The function which calculates metric values for the validation data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IunKbk90eHFB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# define training loss\n",
        "def train_loss(params: hk.Params, state: State, batch: Batch) -> Tuple[jnp.ndarray, State]:\n",
        "    \"\"\"Compute the loss.\"\"\"\n",
        "    inputs, targets = batch\n",
        "    preds, new_state = model.apply(params, state, next(rng_seq), *inputs, True)\n",
        "    loss = bce_with_logits(preds, targets)\n",
        "    return loss, new_state\n",
        "\n",
        "# define training update\n",
        "@jax.jit\n",
        "def update(params: hk.Params, state: State, opt_state: OptState,\n",
        "           batch: Batch) -> Tuple[hk.Params, State, OptState]:\n",
        "    \"\"\"Update the params.\"\"\"\n",
        "    (_, new_state), grads = jax.value_and_grad(train_loss, has_aux=True)(params, state, batch)\n",
        "    updates, new_opt_state = optimizer.update(grads, opt_state)\n",
        "    new_params = optix.apply_updates(params, updates)\n",
        "    return new_params, new_state, new_opt_state\n",
        "\n",
        "# define evaluate metrics\n",
        "@jax.jit\n",
        "def evaluate(params: hk.Params, state: State, batch: Batch) -> jnp.ndarray:\n",
        "    \"\"\"Compute evaluate metrics.\"\"\"\n",
        "    inputs, targets = batch\n",
        "    preds, _ = model.apply(params, state, next(rng_seq), *inputs, False)\n",
        "    loss = bce_with_logits(preds, targets)\n",
        "    return preds, loss, targets"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "njAQ74rIfTtd",
        "colab_type": "text"
      },
      "source": [
        "## Training\n",
        "\n",
        "We set up hyperparamter. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_uG-JBzAfP0P",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# training params\n",
        "seed = 42\n",
        "batch_size = 32\n",
        "early_stop_patience = 15\n",
        "num_epochs = 100\n",
        "\n",
        "# fix seed\n",
        "seed_everything(seed)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0jP62oowiHi3",
        "colab_type": "text"
      },
      "source": [
        "And then, we train our model!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "URFOeWsRiZB2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# initialize some values \n",
        "early_stop = EarlyStopping(patience=early_stop_patience)\n",
        "batch_init_data = (\n",
        "    jnp.zeros((batch_size, *train_dataset.X[0][1].shape)),\n",
        "    jnp.zeros((batch_size, *train_dataset.X[0][0].shape)),\n",
        "    True\n",
        ")\n",
        "params, state = model.init(next(rng_seq), *batch_init_data)\n",
        "opt_state = optimizer.init(params)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xIWhtKozidom",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "158d7923-1913-411f-8541-1e6178695413"
      },
      "source": [
        "print(\"Starting training...\")\n",
        "for epoch in range(num_epochs):\n",
        "    # train\n",
        "    start_time = time.time()\n",
        "    for original_batch in train_dataset.iterbatches(batch_size=batch_size):\n",
        "        batch = collate_fn(original_batch)\n",
        "        params, state, opt_state = update(params, state, opt_state, batch)\n",
        "    epoch_time = time.time() - start_time\n",
        "\n",
        "    # valid\n",
        "    y_score, y_true, valid_loss = [], [], []\n",
        "    for original_batch in valid_dataset.iterbatches(batch_size=batch_size):\n",
        "        batch = collate_fn(original_batch)\n",
        "        logits, loss, targets = evaluate(params, state, batch)\n",
        "        y_score.extend(logits), valid_loss.append(loss), y_true.extend(targets)\n",
        "    score, _ = multi_task_roc_auc_score(np.array(y_true), np.array(y_score))\n",
        "\n",
        "    # log\n",
        "    print(f\"Iter {epoch}/{num_epochs} ({epoch_time:.4f} s) \\\n",
        "            valid loss: {np.mean(valid_loss):.4f} \\\n",
        "            valid roc_auc score: {score:.4f}\")\n",
        "    # check early stopping\n",
        "    early_stop.update(score, (params, state))\n",
        "    if early_stop.is_train_stop:\n",
        "        print(\"Early stopping...\")\n",
        "        break"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Starting training...\n",
            "Iter 0/200 (5.7399 s)             valid loss: 0.2196             valid roc_auc score: 0.5792\n",
            "Iter 1/200 (1.9963 s)             valid loss: 0.2172             valid roc_auc score: 0.5976\n",
            "Iter 2/200 (1.9389 s)             valid loss: 0.2121             valid roc_auc score: 0.5977\n",
            "Iter 3/200 (1.9314 s)             valid loss: 0.2141             valid roc_auc score: 0.6130\n",
            "Iter 4/200 (1.9399 s)             valid loss: 0.2131             valid roc_auc score: 0.6260\n",
            "Iter 5/200 (1.9310 s)             valid loss: 0.2084             valid roc_auc score: 0.6314\n",
            "Iter 6/200 (1.9151 s)             valid loss: 0.2082             valid roc_auc score: 0.6503\n",
            "Iter 7/200 (1.9233 s)             valid loss: 0.2096             valid roc_auc score: 0.6572\n",
            "Iter 8/200 (1.8927 s)             valid loss: 0.2070             valid roc_auc score: 0.6594\n",
            "Iter 9/200 (1.9201 s)             valid loss: 0.2087             valid roc_auc score: 0.6571\n",
            "Iter 10/200 (1.9437 s)             valid loss: 0.2094             valid roc_auc score: 0.6656\n",
            "Iter 11/200 (1.9148 s)             valid loss: 0.2082             valid roc_auc score: 0.6695\n",
            "Iter 12/200 (1.9170 s)             valid loss: 0.2110             valid roc_auc score: 0.6743\n",
            "Iter 13/200 (1.8604 s)             valid loss: 0.2030             valid roc_auc score: 0.6769\n",
            "Iter 14/200 (1.8659 s)             valid loss: 0.2072             valid roc_auc score: 0.6772\n",
            "Iter 15/200 (1.8506 s)             valid loss: 0.2043             valid roc_auc score: 0.6854\n",
            "Iter 16/200 (1.8498 s)             valid loss: 0.2021             valid roc_auc score: 0.6837\n",
            "Iter 17/200 (1.8502 s)             valid loss: 0.2057             valid roc_auc score: 0.6805\n",
            "Iter 18/200 (1.8617 s)             valid loss: 0.2031             valid roc_auc score: 0.6902\n",
            "Iter 19/200 (1.8583 s)             valid loss: 0.2098             valid roc_auc score: 0.6905\n",
            "Iter 20/200 (1.8422 s)             valid loss: 0.2089             valid roc_auc score: 0.6885\n",
            "Iter 21/200 (1.8535 s)             valid loss: 0.2114             valid roc_auc score: 0.6956\n",
            "Iter 22/200 (1.9178 s)             valid loss: 0.2056             valid roc_auc score: 0.6979\n",
            "Iter 23/200 (1.9512 s)             valid loss: 0.2027             valid roc_auc score: 0.7011\n",
            "Iter 24/200 (1.9336 s)             valid loss: 0.2014             valid roc_auc score: 0.7043\n",
            "Iter 25/200 (1.9684 s)             valid loss: 0.2014             valid roc_auc score: 0.7105\n",
            "Iter 26/200 (1.9575 s)             valid loss: 0.1992             valid roc_auc score: 0.7143\n",
            "Iter 27/200 (2.0000 s)             valid loss: 0.2068             valid roc_auc score: 0.6980\n",
            "Iter 28/200 (1.9585 s)             valid loss: 0.2023             valid roc_auc score: 0.7179\n",
            "Iter 29/200 (1.9132 s)             valid loss: 0.2069             valid roc_auc score: 0.7096\n",
            "Iter 30/200 (1.9320 s)             valid loss: 0.1931             valid roc_auc score: 0.7159\n",
            "Iter 31/200 (1.9118 s)             valid loss: 0.2003             valid roc_auc score: 0.7135\n",
            "Iter 32/200 (1.8901 s)             valid loss: 0.1995             valid roc_auc score: 0.7136\n",
            "Iter 33/200 (1.8740 s)             valid loss: 0.2017             valid roc_auc score: 0.7149\n",
            "Iter 34/200 (1.8441 s)             valid loss: 0.1957             valid roc_auc score: 0.7213\n",
            "Iter 35/200 (1.8336 s)             valid loss: 0.1985             valid roc_auc score: 0.7281\n",
            "Iter 36/200 (1.8428 s)             valid loss: 0.1975             valid roc_auc score: 0.7270\n",
            "Iter 37/200 (1.8505 s)             valid loss: 0.2006             valid roc_auc score: 0.7329\n",
            "Iter 38/200 (1.8741 s)             valid loss: 0.1966             valid roc_auc score: 0.7341\n",
            "Iter 39/200 (1.8433 s)             valid loss: 0.1999             valid roc_auc score: 0.7347\n",
            "Iter 40/200 (1.8487 s)             valid loss: 0.1969             valid roc_auc score: 0.7341\n",
            "Iter 41/200 (1.8779 s)             valid loss: 0.1978             valid roc_auc score: 0.7501\n",
            "Iter 42/200 (1.9072 s)             valid loss: 0.1967             valid roc_auc score: 0.7373\n",
            "Iter 43/200 (1.9068 s)             valid loss: 0.1941             valid roc_auc score: 0.7398\n",
            "Iter 44/200 (1.9297 s)             valid loss: 0.1959             valid roc_auc score: 0.7454\n",
            "Iter 45/200 (1.9091 s)             valid loss: 0.1909             valid roc_auc score: 0.7467\n",
            "Iter 46/200 (1.9016 s)             valid loss: 0.1999             valid roc_auc score: 0.7388\n",
            "Iter 47/200 (1.9140 s)             valid loss: 0.1934             valid roc_auc score: 0.7454\n",
            "Iter 48/200 (1.9360 s)             valid loss: 0.1898             valid roc_auc score: 0.7563\n",
            "Iter 49/200 (1.9093 s)             valid loss: 0.1987             valid roc_auc score: 0.7482\n",
            "Iter 50/200 (1.9285 s)             valid loss: 0.2036             valid roc_auc score: 0.7530\n",
            "Iter 51/200 (1.9102 s)             valid loss: 0.1907             valid roc_auc score: 0.7460\n",
            "Iter 52/200 (1.8872 s)             valid loss: 0.1914             valid roc_auc score: 0.7476\n",
            "Iter 53/200 (1.8917 s)             valid loss: 0.1919             valid roc_auc score: 0.7536\n",
            "Iter 54/200 (1.8396 s)             valid loss: 0.1954             valid roc_auc score: 0.7578\n",
            "Iter 55/200 (1.8702 s)             valid loss: 0.1984             valid roc_auc score: 0.7569\n",
            "Iter 56/200 (1.8512 s)             valid loss: 0.2035             valid roc_auc score: 0.7487\n",
            "Iter 57/200 (1.8437 s)             valid loss: 0.1943             valid roc_auc score: 0.7492\n",
            "Iter 58/200 (1.8772 s)             valid loss: 0.1905             valid roc_auc score: 0.7487\n",
            "Iter 59/200 (1.8825 s)             valid loss: 0.1915             valid roc_auc score: 0.7504\n",
            "Iter 60/200 (1.8664 s)             valid loss: 0.1905             valid roc_auc score: 0.7577\n",
            "Iter 61/200 (1.8548 s)             valid loss: 0.1960             valid roc_auc score: 0.7525\n",
            "Iter 62/200 (1.8960 s)             valid loss: 0.1870             valid roc_auc score: 0.7599\n",
            "Iter 63/200 (1.9262 s)             valid loss: 0.1882             valid roc_auc score: 0.7574\n",
            "Iter 64/200 (1.9091 s)             valid loss: 0.2005             valid roc_auc score: 0.7540\n",
            "Iter 65/200 (1.9267 s)             valid loss: 0.1923             valid roc_auc score: 0.7570\n",
            "Iter 66/200 (1.9346 s)             valid loss: 0.1878             valid roc_auc score: 0.7532\n",
            "Iter 67/200 (1.9029 s)             valid loss: 0.1888             valid roc_auc score: 0.7628\n",
            "Iter 68/200 (1.8663 s)             valid loss: 0.1899             valid roc_auc score: 0.7520\n",
            "Iter 69/200 (1.8881 s)             valid loss: 0.1920             valid roc_auc score: 0.7498\n",
            "Iter 70/200 (1.9492 s)             valid loss: 0.1878             valid roc_auc score: 0.7635\n",
            "Iter 71/200 (1.8685 s)             valid loss: 0.1992             valid roc_auc score: 0.7513\n",
            "Iter 72/200 (1.9328 s)             valid loss: 0.1917             valid roc_auc score: 0.7585\n",
            "Iter 73/200 (1.9487 s)             valid loss: 0.1915             valid roc_auc score: 0.7561\n",
            "Iter 74/200 (1.9556 s)             valid loss: 0.1934             valid roc_auc score: 0.7568\n",
            "Iter 75/200 (1.9120 s)             valid loss: 0.1891             valid roc_auc score: 0.7631\n",
            "Iter 76/200 (1.8924 s)             valid loss: 0.1908             valid roc_auc score: 0.7598\n",
            "Iter 77/200 (1.9107 s)             valid loss: 0.1880             valid roc_auc score: 0.7588\n",
            "Iter 78/200 (1.8945 s)             valid loss: 0.1885             valid roc_auc score: 0.7607\n",
            "Iter 79/200 (1.8991 s)             valid loss: 0.1882             valid roc_auc score: 0.7611\n",
            "Iter 80/200 (1.8586 s)             valid loss: 0.1938             valid roc_auc score: 0.7564\n",
            "Iter 81/200 (1.8741 s)             valid loss: 0.1846             valid roc_auc score: 0.7739\n",
            "Iter 82/200 (1.8695 s)             valid loss: 0.1913             valid roc_auc score: 0.7534\n",
            "Iter 83/200 (1.8927 s)             valid loss: 0.1878             valid roc_auc score: 0.7633\n",
            "Iter 84/200 (1.8488 s)             valid loss: 0.1856             valid roc_auc score: 0.7726\n",
            "Iter 85/200 (1.8707 s)             valid loss: 0.1936             valid roc_auc score: 0.7500\n",
            "Iter 86/200 (1.9350 s)             valid loss: 0.1892             valid roc_auc score: 0.7579\n",
            "Iter 87/200 (1.9175 s)             valid loss: 0.1848             valid roc_auc score: 0.7635\n",
            "Iter 88/200 (1.9248 s)             valid loss: 0.1886             valid roc_auc score: 0.7649\n",
            "Iter 89/200 (1.9198 s)             valid loss: 0.1876             valid roc_auc score: 0.7638\n",
            "Iter 90/200 (1.9152 s)             valid loss: 0.1896             valid roc_auc score: 0.7723\n",
            "Iter 91/200 (1.9348 s)             valid loss: 0.1900             valid roc_auc score: 0.7495\n",
            "Iter 92/200 (1.9048 s)             valid loss: 0.1865             valid roc_auc score: 0.7724\n",
            "Iter 93/200 (1.9324 s)             valid loss: 0.1948             valid roc_auc score: 0.7672\n",
            "Iter 94/200 (1.9082 s)             valid loss: 0.1892             valid roc_auc score: 0.7435\n",
            "Iter 95/200 (1.9259 s)             valid loss: 0.1883             valid roc_auc score: 0.7623\n",
            "Iter 96/200 (1.9184 s)             valid loss: 0.1860             valid roc_auc score: 0.7716\n",
            "Early stopping...\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z0lSR6qafB74",
        "colab_type": "text"
      },
      "source": [
        "## Testing\n",
        "\n",
        "Finally, we evaluate the result of test dataset and save best model parametars and states."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zw0yxHMSe5vN",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "b2a2c374-8532-4d15-ad29-2e4d70b98087"
      },
      "source": [
        "y_score, y_true = [], []\n",
        "best_checkpoints = early_stop.best_checkpoints\n",
        "for original_batch in test_dataset.iterbatches(batch_size=batch_size):\n",
        "    batch = collate_fn(original_batch)\n",
        "    logits, _, targets = evaluate(*best_checkpoints, batch)\n",
        "    y_score.extend(logits), y_true.extend(targets)\n",
        "\n",
        "score, scores = multi_task_roc_auc_score(np.array(y_true), np.array(y_score))\n",
        "print(f'Test mean roc_auc score: {score:.4f}')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Test mean roc_auc score: 0.7799\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dEkPhwYZxmqO",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "4fac4f6c-7233-445b-a8ec-bb624be61a31"
      },
      "source": [
        "print(f'Test all roc_auc score: {str(scores)}')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Test all roc_auc score: [0.8177938392703493, 0.9044473800088066, 0.8703401337160338, 0.7781960784313726, 0.6970420913325198, 0.764922568034882, 0.7323701410388718, 0.6976487819919827, 0.7766335892155026, 0.6964186061779865, 0.8184771680247303, 0.8043416858330309]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VEhz4QWKddkI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# save best checkpoints\n",
        "with open('./best_checkpoints.pkl', 'wb') as f:\n",
        "    pickle.dump(best_checkpoints, f)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}