{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "tox21_example.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AzFXv5-BpZoe",
        "colab_type": "text"
      },
      "source": [
        "# Molecule Property Prediction with Tox21 Dataset\n",
        "\n",
        "\n",
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/deepchem/jaxchem//blob/master/notebooks/tox21_exmaple.ipynb)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2bhhW4ekZ_ts",
        "colab_type": "text"
      },
      "source": [
        "## Install packages\n",
        "\n",
        "First, we need to install deepchem for using some useful functions about the Tox21 dataset. (Maybe it will take almost 3 minutes)\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TmtjyHIFrkKi",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 153
        },
        "outputId": "3dfbeded-0dfb-4d2b-c8ed-faed29c1e5b9"
      },
      "source": [
        "%tensorflow_version 1.x\n",
        "!curl -Lo deepchem_installer.py https://raw.githubusercontent.com/deepchem/deepchem/master/scripts/colab_install.py\n",
        "import deepchem_installer\n",
        "%time deepchem_installer.install(version='2.3.0')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "TensorFlow 1.x selected.\n",
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "100  3477  100  3477    0     0  25014      0 --:--:-- --:--:-- --:--:-- 25014\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "add /root/miniconda/lib/python3.6/site-packages to PYTHONPATH\n",
            "deepchem is already installed\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "CPU times: user 1.65 ms, sys: 920 Âµs, total: 2.57 ms\n",
            "Wall time: 2.18 ms\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "askrOx-kbXGk",
        "colab_type": "text"
      },
      "source": [
        "And then, we install jaxchem with some dependencies   \n",
        "**Caution** : After running the following commands, you need to restart the session. If you don't restart the session, maybe you will face an error."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Pcur1OJXIAJz",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "d19e0d08-3868-413f-d8d1-8fb0a45fffa0"
      },
      "source": [
        "!pip install -q dm-haiku==0.0.1 typing-extensions==3.7.4.2  git+https://github.com/deepchem/jaxchem"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "  Building wheel for jaxchem (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CJd7o5qNcps5",
        "colab_type": "text"
      },
      "source": [
        "## Import modules\n",
        "\n",
        "If we face the error `ImportError: cannot import name 'Literal'\n",
        "`, we should restart the session of this notebook."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2XBbmpnacoYa",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 207
        },
        "outputId": "a33930e3-7e16-4e72-f28d-ea46ca87bacd"
      },
      "source": [
        "import os\n",
        "import time\n",
        "import random\n",
        "import pickle\n",
        "import argparse\n",
        "from typing import Any, Tuple\n",
        "\n",
        "import jax\n",
        "import numpy as np\n",
        "import haiku as hk\n",
        "import jax.numpy as jnp\n",
        "from jax.experimental import optix\n",
        "from sklearn.metrics import roc_auc_score\n",
        "\n",
        "\n",
        "from deepchem.molnet import load_tox21\n",
        "from jaxchem.models import PadGCNPredicator, clipped_sigmoid\n",
        "from jaxchem.utils import EarlyStopping\n",
        "\n",
        "\n",
        "# type definition\n",
        "Batch = Tuple[Tuple[np.ndarray, np.ndarray], np.ndarray]\n",
        "State, OptState = Any, Any\n",
        "\n",
        "# tox21 tasks\n",
        "task_names = ['NR-AR', 'NR-AR-LBD', 'NR-AhR', 'NR-Aromatase', 'NR-ER', 'NR-ER-LBD',\n",
        "              'NR-PPAR-gamma', 'SR-ARE', 'SR-ATAD5', 'SR-HSE', 'SR-MMP', 'SR-p53']"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.\n",
            "  warnings.warn(msg, category=FutureWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:\n",
            "The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
            "For more information, please see:\n",
            "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
            "  * https://github.com/tensorflow/addons\n",
            "  * https://github.com/tensorflow/io (for I/O related ops)\n",
            "If you depend on functionality not listed there, please file an issue.\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Czw4zaO6dYWG",
        "colab_type": "text"
      },
      "source": [
        "## Download Tox21 dateset\n",
        "\n",
        "we download the Tox21 dataset which were preprocessed.  In this example, we should use the `AdjacencyConv` featurizer because `PadGCNPredicator` depends on the pad pattern GCN which use the adjacency matrix to represent node connections\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WtlinRn-f_GL",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "4bc6e428-4f73-411d-a798-9cd8f4e92b6a"
      },
      "source": [
        "# load tox21 dataset\n",
        "tox21_tasks, tox21_datasets, _ = load_tox21(featurizer='AdjacencyConv', reload=True)\n",
        "train_dataset, valid_dataset, test_dataset = tox21_datasets"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Loading dataset from disk.\n",
            "Loading dataset from disk.\n",
            "Loading dataset from disk.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VKaZhd89dEz8",
        "colab_type": "text"
      },
      "source": [
        "## Define some utilities"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "55ycWMB1dBE5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def seed_everything(seed: int = 42):\n",
        "    random.seed(seed)\n",
        "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
        "    np.random.seed(seed)\n",
        "\n",
        "\n",
        "def collate_fn(original_batch: Any, task_index: int) -> Batch:\n",
        "    \"\"\"Make batch data for PadGCN model inputs.\"\"\"\n",
        "    inputs, targets, _, _ = original_batch\n",
        "    node_feats = np.array([inputs[i][1] for i in range(len(inputs))])\n",
        "    adj = np.array([inputs[i][0] for i in range(len(inputs))])\n",
        "    targets = targets[:, task_index]\n",
        "    return ((node_feats, adj), targets)\n",
        "\n",
        "def binary_cross_entropy(logits: jnp.ndarray, targets: jnp.ndarray) -> jnp.ndarray:\n",
        "    \"\"\"Compute binary cross entropy loss.\"\"\"\n",
        "    return -jnp.mean(targets * jnp.log(logits) + (1.0 - targets) * jnp.log(1.0 - logits))"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hoqID21XceTH",
        "colab_type": "text"
      },
      "source": [
        "## Setup model and optimizer\n",
        "\n",
        "We define the forward function using `GCNPredicator` which JAXChem provides. In this case, our task is a classification, so we modify the output of  `GCNPredicator` using a sigmoid function. After defining the forward function, we create the model instance by using `haiku.transform_with_state`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GKZl6BKBdzzz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "rng_seq = hk.PRNGSequence(1234)\n",
        "\n",
        "# model params\n",
        "in_feats = train_dataset.X[0][1].shape[1]\n",
        "hidden_feats = [64, 64, 64]\n",
        "activation, batch_norm, dropout = None, None, None  # use default\n",
        "predicator_hidden_feats = 32\n",
        "pooling_method = 'mean'\n",
        "predicator_dropout = None  # use default\n",
        "n_out = 1  # binary classification\n",
        "\n",
        "def forward(node_feats: jnp.ndarray, adj: jnp.ndarray, is_training: bool) -> jnp.ndarray:\n",
        "    \"\"\"Forward application of the GCN.\"\"\"\n",
        "    model = PadGCNPredicator(in_feats=in_feats, hidden_feats=hidden_feats, activation=activation,\n",
        "                                              batch_norm=batch_norm, dropout=dropout, pooling_method=pooling_method,\n",
        "                                              predicator_hidden_feats=predicator_hidden_feats,\n",
        "                                              predicator_dropout=predicator_dropout, n_out=n_out)\n",
        "    preds = model(node_feats, adj, is_training)\n",
        "    logits = clipped_sigmoid(preds)\n",
        "    return logits\n",
        "\n",
        "# we use haiku\n",
        "model = hk.transform_with_state(forward)"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zz6mxngGaxx4",
        "colab_type": "text"
      },
      "source": [
        "And then, we also create the optimizer instance. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3QURfCI7eWD1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# optimizer params\n",
        "lr = 0.001\n",
        "optimizer = optix.adam(learning_rate=lr)"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9H8SWjyfdeJX",
        "colab_type": "text"
      },
      "source": [
        "## Define loss, update and evaluate function\n",
        "\n",
        "Using the model and optimizer instance, we define the following functions. These functions are used in a training loop, so we add `@jax.jit` of the decorator to improve the performance.\n",
        "\n",
        "- The function which calculates a loss value\n",
        "- The function which updates parameters\n",
        "- The function which calculates metric values for the validation data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IunKbk90eHFB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# define training loss\n",
        "def train_loss(params: hk.Params, state: State, batch: Batch) -> Tuple[jnp.ndarray, State]:\n",
        "    \"\"\"Compute the loss.\"\"\"\n",
        "    inputs, targets = batch\n",
        "    logits, new_state = model.apply(params, state, next(rng_seq), *inputs, True)\n",
        "    loss = binary_cross_entropy(logits, targets)\n",
        "    return loss, new_state\n",
        "\n",
        "# define training update\n",
        "@jax.jit\n",
        "def update(params: hk.Params, state: State, opt_state: OptState,\n",
        "           batch: Batch) -> Tuple[hk.Params, State, OptState]:\n",
        "    \"\"\"Update the params.\"\"\"\n",
        "    (_, new_state), grads = jax.value_and_grad(train_loss, has_aux=True)(params, state, batch)\n",
        "    updates, new_opt_state = optimizer.update(grads, opt_state)\n",
        "    new_params = optix.apply_updates(params, updates)\n",
        "    return new_params, new_state, new_opt_state\n",
        "\n",
        "# define evaluate metrics\n",
        "@jax.jit\n",
        "def evaluate(params: hk.Params, state: State, batch: Batch) -> jnp.ndarray:\n",
        "    \"\"\"Compute evaluate metrics.\"\"\"\n",
        "    inputs, targets = batch\n",
        "    logits, _ = model.apply(params, state, next(rng_seq), *inputs, False)\n",
        "    loss = binary_cross_entropy(logits, targets)\n",
        "    return logits, loss, targets"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "njAQ74rIfTtd",
        "colab_type": "text"
      },
      "source": [
        "## Training\n",
        "\n",
        "We set up hyperparamter. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_uG-JBzAfP0P",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# training params\n",
        "seed = 42\n",
        "batch_size = 32\n",
        "early_stop_patience = 15\n",
        "num_epochs = 50\n",
        "task = 'NR-AR'\n",
        "\n",
        "# fix seed\n",
        "seed_everything(seed)"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0jP62oowiHi3",
        "colab_type": "text"
      },
      "source": [
        "And then, we train our model!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "URFOeWsRiZB2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# initialize some values \n",
        "task_index = tox21_tasks.index(task)\n",
        "early_stop = EarlyStopping(patience=early_stop_patience)\n",
        "batch_init_data = (\n",
        "    np.zeros((batch_size, *train_dataset.X[0][1].shape)),\n",
        "    np.zeros((batch_size, *train_dataset.X[0][0].shape)),\n",
        "    True\n",
        ")\n",
        "params, state = model.init(next(rng_seq), *batch_init_data)\n",
        "opt_state = optimizer.init(params)"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xIWhtKozidom",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 510
        },
        "outputId": "04e06ef8-e06c-43b0-993e-0c7dc345896b"
      },
      "source": [
        "print(\"Starting training...\")\n",
        "for epoch in range(num_epochs):\n",
        "    # train\n",
        "    start_time = time.time()\n",
        "    for original_batch in train_dataset.iterbatches(batch_size=batch_size):\n",
        "        batch = collate_fn(original_batch, task_index)\n",
        "        params, state, opt_state = update(params, state, opt_state, batch)\n",
        "    epoch_time = time.time() - start_time\n",
        "\n",
        "    # valid\n",
        "    y_score, y_true, valid_loss = [], [], []\n",
        "    for original_batch in valid_dataset.iterbatches(batch_size=batch_size):\n",
        "        batch = collate_fn(original_batch, task_index)\n",
        "        logits, loss, targets = evaluate(params, state, batch)\n",
        "        y_score.extend(logits), valid_loss.append(loss), y_true.extend(targets)\n",
        "    score = roc_auc_score(y_true, y_score)\n",
        "    # log\n",
        "    print(f\"Iter {epoch}/{num_epochs} ({epoch_time:.4f} s) valid loss: {np.mean(valid_loss):.4f} \\\n",
        "        valid roc_auc score: {score:.4f}\")\n",
        "\n",
        "    # check early stopping\n",
        "    early_stop.update(score, (params, state))\n",
        "    if early_stop.is_train_stop:\n",
        "        print(\"Early stopping...\")\n",
        "        break"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Starting training...\n",
            "Iter 0/50 (4.5257 s) valid loss: 0.1609         valid roc_auc score: 0.6886\n",
            "Iter 1/50 (1.8490 s) valid loss: 0.1611         valid roc_auc score: 0.6843\n",
            "Iter 2/50 (1.6758 s) valid loss: 0.1568         valid roc_auc score: 0.6875\n",
            "Iter 3/50 (1.6675 s) valid loss: 0.1616         valid roc_auc score: 0.6901\n",
            "Iter 4/50 (1.6652 s) valid loss: 0.1560         valid roc_auc score: 0.6476\n",
            "Iter 5/50 (1.6780 s) valid loss: 0.1607         valid roc_auc score: 0.7002\n",
            "Iter 6/50 (1.6586 s) valid loss: 0.1590         valid roc_auc score: 0.6935\n",
            "Iter 7/50 (1.6643 s) valid loss: 0.1713         valid roc_auc score: 0.6992\n",
            "Iter 8/50 (1.6635 s) valid loss: 0.1559         valid roc_auc score: 0.7040\n",
            "Iter 9/50 (1.6957 s) valid loss: 0.1562         valid roc_auc score: 0.7161\n",
            "Iter 10/50 (1.6514 s) valid loss: 0.1612         valid roc_auc score: 0.6659\n",
            "Iter 11/50 (1.6497 s) valid loss: 0.1569         valid roc_auc score: 0.7249\n",
            "Iter 12/50 (1.6658 s) valid loss: 0.1563         valid roc_auc score: 0.7110\n",
            "Iter 13/50 (1.6608 s) valid loss: 0.1559         valid roc_auc score: 0.7206\n",
            "Iter 14/50 (1.6807 s) valid loss: 0.1559         valid roc_auc score: 0.6975\n",
            "Iter 15/50 (1.6734 s) valid loss: 0.1559         valid roc_auc score: 0.7173\n",
            "Iter 16/50 (1.6749 s) valid loss: 0.1590         valid roc_auc score: 0.7024\n",
            "Iter 17/50 (1.6939 s) valid loss: 0.1565         valid roc_auc score: 0.7055\n",
            "Iter 18/50 (1.6543 s) valid loss: 0.1558         valid roc_auc score: 0.7208\n",
            "Iter 19/50 (1.6462 s) valid loss: 0.1558         valid roc_auc score: 0.7086\n",
            "Iter 20/50 (1.6195 s) valid loss: 0.1663         valid roc_auc score: 0.7039\n",
            "Iter 21/50 (1.6588 s) valid loss: 0.1606         valid roc_auc score: 0.7072\n",
            "Iter 22/50 (1.6555 s) valid loss: 0.1607         valid roc_auc score: 0.7192\n",
            "Iter 23/50 (1.6401 s) valid loss: 0.1657         valid roc_auc score: 0.7156\n",
            "Iter 24/50 (1.6640 s) valid loss: 0.1611         valid roc_auc score: 0.7110\n",
            "Iter 25/50 (1.6296 s) valid loss: 0.1559         valid roc_auc score: 0.7083\n",
            "Iter 26/50 (1.6674 s) valid loss: 0.1605         valid roc_auc score: 0.7055\n",
            "Early stopping...\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z0lSR6qafB74",
        "colab_type": "text"
      },
      "source": [
        "## Testing\n",
        "\n",
        "Finally, we evaluate the result of test dataset and save best model parametars and states."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zw0yxHMSe5vN",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "c5c42706-0818-470b-af8f-c231a3af223e"
      },
      "source": [
        "y_score, y_true = [], []\n",
        "best_checkpoints = early_stop.best_checkpoints\n",
        "for original_batch in test_dataset.iterbatches(batch_size=batch_size):\n",
        "    batch = collate_fn(original_batch, task_index)\n",
        "    logits, _, targets = evaluate(*best_checkpoints, batch)\n",
        "    y_score.extend(logits), y_true.extend(targets)\n",
        "\n",
        "score = roc_auc_score(y_true, y_score)\n",
        "print(f'Test roc_auc score: {score:.4f}')"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Test roc_auc score: 0.7349\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VEhz4QWKddkI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# save best checkpoints\n",
        "with open('./best_checkpoints.pkl', 'wb') as f:\n",
        "    pickle.dump(best_checkpoints, f)"
      ],
      "execution_count": 13,
      "outputs": []
    }
  ]
}